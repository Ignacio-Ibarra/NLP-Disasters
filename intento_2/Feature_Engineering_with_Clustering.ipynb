{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Engineering with Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpcw5go7jF38",
        "colab_type": "text"
      },
      "source": [
        "The basic idea of this notebook is generate clusters with K-Means or HDBScan and use the number of the cluster as a new feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j6PsO_4jASa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmYyaNtCt7mW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = pd.read_csv('https://raw.githubusercontent.com/Ignacio-Ibarra/NLP-Disasters/intento_1_nacho/intento_1/train_no_duplicates_with_word_classes.csv',\\\n",
        "                         sep=',', usecols = ['url_count',\n",
        "       'hashtag_count', 'mention_count', 'digits_count', 'characters_count',\n",
        "       'characters_count_clean', 'characters_count_clean_sw', 'word_count',\n",
        "       'word_count_clean', 'word_count_clean_sw', 'avg_word_len',\n",
        "       'avg_word_len_clean_sw', 'PRP$', 'NNS', 'VBP', 'DT', 'NN', 'IN', 'MD',\n",
        "       'VB', 'JJ', 'PRP', 'JJS', 'VBD', 'TO', 'VBG', 'VBN', 'CC', 'CD', 'RB',\n",
        "       'EX', 'VBZ', 'WP', 'RP', 'JJR', 'WRB', '$', 'WDT', 'NNP', 'RBR', 'RBS',\n",
        "       'PDT', 'SYM', 'FW', 'UH', 'X', 'WP$', 'target'],\\\n",
        "       dtype = {'PRP$': 'int64', 'NNS': 'int64', 'VBP': 'int64', 'DT': 'int64', 'NN': 'int64', 'IN': 'int64', 'MD': 'int64',\n",
        "       'VB': 'int64', 'JJ': 'int64', 'PRP': 'int64', 'JJS': 'int64', 'VBD': 'int64', 'TO': 'int64', 'VBG': 'int64', 'VBN': 'int64',\\\n",
        "       'CC': 'int64', 'CD': 'int64', 'RB': 'int64','EX': 'int64', 'VBZ': 'int64', 'WP': 'int64', 'RP': 'int64', 'JJR': 'int64',\\\n",
        "       'WRB': 'int64', '$': 'int64', 'WDT': 'int64', 'NNP': 'int64', 'RBR': 'int64', 'RBS': 'int64','PDT': 'int64', 'SYM': 'int64',\\\n",
        "       'FW': 'int64', 'UH': 'int64', 'X': 'int64', 'WP$': 'int64', 'target': 'int64'})\n",
        "\n",
        "\n",
        "\n",
        "data_test = pd.read_csv('https://raw.githubusercontent.com/Ignacio-Ibarra/NLP-Disasters/intento_1_nacho/intento_1/featured_test.csv',sep=',')\n",
        "data_test = data_test[data_test.columns[1:]]\n",
        "data_test.columns = ['id', 'keyword', 'location', 'text', 'text_clean', 'text_clean_no_sw',\n",
        "       'url_count', 'hashtag_count', 'mention_count', 'digits_count',\n",
        "       'characters_count', 'characters_count_clean',\n",
        "       'characters_count_clean_sw', 'word_count', 'word_count_clean',\n",
        "       'word_count_clean_sw', 'avg_word_len', 'avg_word_len_clean_sw',\n",
        "       'tokenized_text', 'pos_tagged_text', 'pos_tagg_counts', 'RB', 'VBD',\n",
        "       'DT', 'JJ', 'NN', 'IN', 'VBZ', 'NNS', 'VBP', 'EX', 'VBG', 'VB', 'PRP',\n",
        "       'CD', 'CC', 'MD', 'RBR', 'WRB', 'WP', 'VBN', 'RP', 'PRP$', '$', 'TO',\n",
        "       'NNP', 'WDT', 'PDT', 'JJS', 'JJR', 'FW', 'RBS', 'X', 'UH']\n",
        "id = list(data_test.id)\n",
        "x_test_numeric = data_test[['url_count', 'hashtag_count', 'mention_count', 'digits_count',\n",
        "       'characters_count', 'characters_count_clean',\n",
        "       'characters_count_clean_sw', 'word_count', 'word_count_clean',\n",
        "       'word_count_clean_sw', 'avg_word_len', 'avg_word_len_clean_sw',\n",
        "       'RB', 'VBD','DT', 'JJ', 'NN', 'IN', 'VBZ', 'NNS', 'VBP', 'EX', 'VBG', 'VB', 'PRP',\n",
        "       'CD', 'CC', 'MD', 'RBR', 'WRB', 'WP', 'VBN', 'RP', 'PRP$', '$', 'TO',\n",
        "       'NNP', 'WDT', 'PDT', 'JJS', 'JJR', 'FW', 'RBS', 'X', 'UH']]\n",
        "\n",
        "drop = []\n",
        "for i in list(data_test.columns): \n",
        "  if i not in list(data_train.columns): \n",
        "    drop.append(i)\n",
        "\n",
        "data_test.drop(columns= drop, inplace=True)\n",
        "\n",
        "drop = []\n",
        "for i in list(data_train.columns): \n",
        "  if i not in list(data_test.columns) and i != 'target':\n",
        "    drop.append(i)\n",
        "\n",
        "data_train.drop(columns=drop, inplace=True)\n",
        "\n",
        "X_train, y_train = data_train.iloc[:,1:], data_train['target']\n",
        "X_test = data_test\n",
        "\n",
        "X_train.fillna(value=0, inplace = True)\n",
        "X_test.fillna(value=0, inplace = True)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzbsfErB2vlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = X_test[['url_count', 'hashtag_count', 'mention_count', 'digits_count',\n",
        "       'characters_count', 'characters_count_clean',\n",
        "       'characters_count_clean_sw', 'word_count', 'word_count_clean',\n",
        "       'word_count_clean_sw', 'avg_word_len', 'avg_word_len_clean_sw', 'PRP$',\n",
        "       'NNS', 'VBP', 'DT', 'NN', 'IN', 'MD', 'VB', 'JJ', 'PRP', 'JJS', 'VBD',\n",
        "       'TO', 'VBG', 'VBN', 'CC', 'CD', 'RB', 'EX', 'VBZ', 'WP', 'RP', 'JJR',\n",
        "       'WRB', '$', 'WDT', 'NNP', 'RBR', 'RBS', 'PDT', 'FW', 'UH', 'X']]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWWJLyH7bU5C",
        "colab_type": "text"
      },
      "source": [
        "# Using HDBScan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0xLOZSplwWL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "64f41b6b-a875-4f90-f302-2f3bde71ae40"
      },
      "source": [
        "!pip install hdbscan"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hdbscan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/2f/2423d844072f007a74214c1adc46260e45f034bb1679ccadfbb8a601f647/hdbscan-0.8.26.tar.gz (4.7MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7MB 4.5MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.29.21)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from hdbscan) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from hdbscan) (0.16.0)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.26-cp36-cp36m-linux_x86_64.whl size=2305918 sha256=8af009671b55f14f0663578b821cbac0ec03742c6c607024c9599fb2181a5b6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/38/41/372f034d8abd271ef7787a681e0a47fc05d472683a7eb088ed\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP9BD9q18eMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train['istrain'] = 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2LcppaN8vp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e0f6a1d9-0d0b-45cc-c47c-bb48a17af30b"
      },
      "source": [
        "X_test['istrain'] = 0"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHdE0X_79F7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "appended = X_train.append(X_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37acmv4smGlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hdbscan"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yikhOnVImS68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clusterer = hdbscan.HDBSCAN()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce0Kz_3QmoYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "531b8bb4-a515-45ef-9f9b-7c9a8fe9278e"
      },
      "source": [
        "clusterer.fit(appended)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HDBSCAN(algorithm='best', allow_single_cluster=False, alpha=1.0,\n",
              "        approx_min_span_tree=True, cluster_selection_epsilon=0.0,\n",
              "        cluster_selection_method='eom', core_dist_n_jobs=4,\n",
              "        gen_min_span_tree=False, leaf_size=40,\n",
              "        match_reference_implementation=False, memory=Memory(location=None),\n",
              "        metric='euclidean', min_cluster_size=5, min_samples=None, p=None,\n",
              "        prediction_data=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEoFbVle7pPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_labels_hdbscan_appended = clusterer.labels_"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVs811sR9woM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "appended['cluster_labels_hdbscan'] = cluster_labels_hdbscan_appended"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya3vO2cs95vE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = appended[appended['istrain']==1]\n",
        "X_test = appended[appended['istrain']==0]\n",
        "X_train = X_train.drop(columns='istrain')\n",
        "X_test = X_test.drop(columns='istrain')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCFN6mtq-i1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.to_csv('X_train.csv', sep=',', index=False)\n",
        "X_test.to_csv('X_test.csv', sep=',', index=False)"
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}