{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyL6FMUJs0kD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2263316-532d-4d16-833f-a727cf685af7"
      },
      "source": [
        "!pip install pyspellchecker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt-gp1qks6f7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "367ed303-fb3a-4b38-92ff-4ef5ed2023e3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "import string\n",
        "from spellchecker import SpellChecker\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk2OXGdftOoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = pd.read_csv(r'https://raw.githubusercontent.com/Ignacio-Ibarra/NLP-Disasters/intento_3_nachos/intento_3/train_clean.csv',usecols=['id','text'])\n",
        "y_train = pd.read_csv(r'https://raw.githubusercontent.com/Ignacio-Ibarra/NLP-Disasters/intento_3_nachos/intento_3/train_clean.csv',usecols= ['target'])\n",
        "x_test = pd.read_csv(r'https://raw.githubusercontent.com/Ignacio-Ibarra/NLP-Disasters/intento_3_nachos/intento_3/test_clean.csv', usecols = ['id','text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUjz7k_w1mjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train['istrain'] = 1\n",
        "x_test['istrain'] = 0\n",
        "complete = x_train.append(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCtRpZrM14k5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete['text'] = complete[\"text\"].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH1QA00u2Njl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete[\"text_lower\"] = complete[\"text\"].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yw78-JD2XZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "complete[\"text_remove_punct\"] = complete[\"text_lower\"].apply(lambda text: remove_punctuation(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovx4WPv23FDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "complete[\"text_wo_stop\"] = complete[\"text_remove_punct\"].apply(lambda text: remove_stopwords(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLGcGubK3o5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "complete[\"text_wo_emojis\"] = complete[\"text_wo_stop\"].apply(lambda text: remove_emoji(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NopIRlRP7i6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_html(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "complete[\"text_wo_html\"] = complete[\"text_wo_emojis\"].apply(lambda text: remove_html(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHq_TXnT7yjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing chat words\n",
        "\n",
        "#This is a pandas df with many types of \"chat words\"\n",
        "slang = pd.read_csv(r'/content/slang.txt', sep = '=', names = ['slang','meaning'])\n",
        "\n",
        "def replace_chat_word(text): \n",
        "  line = []\n",
        "  for i in text.split(): \n",
        "    if i in slang.slang.to_list(): \n",
        "      i = slang.loc[slang.slang == i,'meaning'].values[0].lower()\n",
        "      line.append(i)\n",
        "    else: \n",
        "      line.append(i)\n",
        "  return str(\" \".join(line))\n",
        "\n",
        "complete['text_no_chat_word'] = complete[\"text_wo_html\"].apply(lambda x: replace_chat_word(x))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWUg9NvyShA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import emot\n",
        "from emot.core import emoji\n",
        "from emot.core import emoticons\n",
        "from emot.emo_unicode import EMO_UNICODE\n",
        "from emot.emo_unicode import UNICODE_EMO\n",
        "from emot.emo_unicode import EMOTICONS"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVW_YUkwW2Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_emoticons(text):\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
        "    return emoticon_pattern.sub(r'', text)\n",
        "\n",
        "complete['text_clean']=complete['text_no_chat_word'].apply(lambda x: remove_emoticons(x))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRbKKkYmXX5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete = complete[['id', 'text','text_clean', 'istrain']]\n",
        "x_train = complete.loc[complete.istrain == 1,:]\n",
        "x_test = complete.loc[complete.istrain == 0,:]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQxpqHDNYEeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train.to_csv('train_clean.csv', index=False)\n",
        "x_test.to_csv('test_clean.csv', index=False)\n",
        "y_train.to_csv('targets.csv', index=False)"
      ],
      "execution_count": 89,
      "outputs": []
    }
  ]
}