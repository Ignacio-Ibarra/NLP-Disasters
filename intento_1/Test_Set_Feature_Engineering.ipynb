{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test Set Feature Engineering.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzo5rP29diXCaE1R7zG73J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ignacio-Ibarra/NLP-Disasters/blob/intento_1_nacho/intento_1/Test_Set_Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn6JCT2JEtTI",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we're gonna generate same features that we had made with the train set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUSPibbAEP5k",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMcmZLF-EC6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "3abc4a37-2231-41e9-bdfa-6440043b9c9d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('ggplot')\n",
        "from wordcloud import WordCloud\n",
        "import PIL\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop=set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import string\n",
        "import re\n",
        "from PIL import Image    # to import the image\n",
        "from google.colab import files\n",
        "from nltk.util import ngrams # function for making ngrams\n",
        "import collections\n",
        "import plotly.express as px\n",
        "\n",
        "sns.set_context('paper',font_scale=1.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#para suprimir notacion cientifica en los outputs\n",
        "pd.options.display.float_format='{:20,.1f}'.format\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itF755mOEI7Z",
        "colab_type": "text"
      },
      "source": [
        "# Reading csv file from GitHub and creating DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJsPJe11ESlU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f1c2d857-31e4-4516-cd01-16cbe8113f63"
      },
      "source": [
        "#Reading test set\n",
        "\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/Ignacio-Ibarra/NLP-Disasters/master/csv/test.csv', sep=',')\n",
        "test = test[0:]\n",
        "test.info()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3263 entries, 0 to 3262\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        3263 non-null   int64 \n",
            " 1   keyword   3237 non-null   object\n",
            " 2   location  2158 non-null   object\n",
            " 3   text      3263 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 102.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APZLqkZxGfKj",
        "colab_type": "text"
      },
      "source": [
        "# Filling nulls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag40BA2qGk80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test['keyword']=test['keyword'].fillna('no_keyword')\n",
        "test['location']=test['location'].fillna('no_location')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1AJaHq-Mr_a",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering to Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5q9ENWeM2Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanText(inputString):\n",
        "  text = re.sub(r'http\\S+', '', inputString)  \n",
        "  text2 = re.sub(r'@\\S+', '', text)\n",
        "  text3 = re.sub(r'&\\S+', '', text2)\n",
        "  text4 = re.sub(r'Ã\\S+', '', text3)\n",
        "  text5 = re.sub(r'www\\S+','',text4)\n",
        "  text5 = re.sub(r'www\\S+','',text4)\n",
        "  text6 = re.sub(r'\\x89Û\\S+','',text4)\n",
        "\n",
        "  return (text6)\n",
        " \n",
        "test['text_clean'] = test['text'].apply(lambda x: cleanText(x))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s8w6mB0NIZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decontracted(inputString):\n",
        "\n",
        "    phrase = re.sub(r\"won't\", \"will not\", inputString)\n",
        "    phrase = re.sub(r\"couldn't\", \"could not\", inputString)\n",
        "    phrase = re.sub(r\"can't\", \"can not\", inputString)\n",
        "    phrase = re.sub(r\"don't\", \"do not\", inputString)\n",
        "    phrase = re.sub(r\"doesn't\", \"does not\", inputString)\n",
        "    phrase = re.sub(r\"isn't\", \"is not\", inputString)\n",
        "    phrase = re.sub(r\"mustn't\", \"must not\", inputString)\n",
        "    phrase = re.sub(r\"shouldn't\", \"was not\", inputString)\n",
        "    phrase = re.sub(r\"wasn't\", \"was not\", inputString)\n",
        "    phrase = re.sub(r\"won't\", \"will not\", inputString)\n",
        "    \n",
        "    # general\n",
        "    phrase = re.sub(r\" 's\", \" \\\"s\", phrase)\n",
        "    phrase = re.sub(r\" 't\", \" \\\"t\", phrase)\n",
        "    phrase = re.sub(r\" 'd\", \" \\\"d\", phrase)\n",
        "    phrase = re.sub(r\" 're\", \" \\\"re\", phrase)\n",
        "    phrase = re.sub(r\" 'll\", \" \\\"ll\", phrase)\n",
        "    phrase = re.sub(r\" 've\", \" \\\"ve\", phrase)\n",
        "    phrase = re.sub(r\" 'm\", \" \\\"m\", phrase)\n",
        "\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "test['text_clean'] = test['text_clean'].apply(lambda x: decontracted(x))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMBHg90HNZ8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deletePunctuation(inputString):\n",
        "  aux=string.punctuation\n",
        "  special_characters='ÃŒ©‰¥¼ªû'\n",
        "  #puncts=aux.translate(str.maketrans(\"#@:/.\",\"     \"))\n",
        "  auxstring=\" \" * len(aux)\n",
        "  auxstring2=\" \" * len(special_characters)\n",
        "  outputString=inputString.translate(str.maketrans(aux,auxstring))\n",
        "  outputString=outputString.translate(str.maketrans(special_characters,auxstring2))\n",
        "  return outputString\n",
        "\n",
        "test['text_clean'] = test['text_clean'].apply(lambda x:deletePunctuation(x))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwoLSANPNdLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deleteStopwords(inputString):\n",
        "  inputString= inputString.lower()\n",
        "  text_tokens = word_tokenize(inputString)\n",
        "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
        "  filtered_sentence = (\" \").join(tokens_without_sw)\n",
        "  return filtered_sentence\n",
        "\n",
        "test['text_clean_no_sw'] = test['text_clean'].apply(lambda x:deleteStopwords(x))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNnTWC9qN0jr",
        "colab_type": "text"
      },
      "source": [
        "### Tweets text features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VivUCxjOH8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def url_count(a):\n",
        "  a=a.split()\n",
        "  i=0\n",
        "  count=0\n",
        "  while i<len(a):\n",
        "    if 'http:' in a[i] or 'www.' in a[i]:\n",
        "      count=count+1\n",
        "    i=i+1\n",
        "  return count \n",
        "\n",
        "def mention_count(a):\n",
        "  a=a.split()\n",
        "  i=0\n",
        "  count=0\n",
        "  while i<len(a):\n",
        "    \n",
        "    if '@' in a[i]:\n",
        "      count=count+1\n",
        "    i=i+1\n",
        "  return count \n",
        "\n",
        "def hashtag_count(a):\n",
        "  a=a.split()\n",
        "  i=0\n",
        "  count=0\n",
        "  while i<len(a):\n",
        "    \n",
        "    if '#' in a[i]:\n",
        "      count=count+1\n",
        "    i=i+1\n",
        "  return count  \n",
        "\n",
        "def numbers_count(inputString):\n",
        "  count=0\n",
        "  i=0\n",
        "  inputString=inputString.split()\n",
        "  while i < len(inputString):\n",
        "    if inputString[i].isdigit() == True:\n",
        "      count=count+1\n",
        "    i=i+1\n",
        "  return count\n",
        "\n",
        "test['url_count'] = test['text'].apply(lambda x: url_count(x))\n",
        "test['hashtag_count'] = test['text'].apply(lambda x: hashtag_count(x))\n",
        "test['mention_count'] = test['text'].apply(lambda x: mention_count(x))\n",
        "test['digits_count'] = test['text_clean'].apply(lambda x: numbers_count(x))\n",
        "test['characters_count'] = test['text'].apply(lambda x: len(x))\n",
        "test['characters_count_clean'] = test['text_clean'].apply(lambda x: len(x))\n",
        "test['characters_count_clean_sw'] = test['text_clean_no_sw'].apply(lambda x: len(x))\n",
        "test['word_count'] = test['text'].apply(lambda x: len(x.split()))\n",
        "test['word_count_clean'] = test['text_clean'].apply(lambda x: len(x.split()))\n",
        "test['word_count_clean_sw'] = test['text_clean_no_sw'].apply(lambda x: len(x.split()))\n",
        "test['avg_word_len']= test['characters_count']/ test['word_count']\n",
        "test['avg_word_len_clean_sw']= test['characters_count_clean_sw']/ test['word_count_clean_sw']"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FtYUV9kOs4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Obtenemos el token de cada texto. \n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tt = TweetTokenizer()\n",
        "test['tokenized_text']=test['text_clean'].apply(lambda x: x.lower()).apply(tt.tokenize) \n",
        "\n",
        "#Para cada token obtenemos el tag (clase de palabra)\n",
        "test['pos_tagged_text'] = test['tokenized_text'].apply(nltk.Text).apply(nltk.pos_tag)\n",
        "\n",
        "#En cada fila contamos la frecuencia de las clases de palabras\n",
        "from collections import Counter\n",
        "test['pos_tagg_counts']=test['pos_tagged_text'].apply(lambda x: Counter(tag for word,tag in x))\n",
        "\n",
        "#Creamos un dataframe con la misma cantidad de filas que el dataframe con el que venimos trabajando. \n",
        "#Las columnas serán todas las clases de palabras existentes en la librería nltk que estén dentro de nuestras frecuencias. \n",
        "data = test['pos_tagg_counts']\n",
        "word_classes_df = pd.DataFrame.from_records(data, columns = data.sum().keys())\n",
        "\n",
        "#Dicho dataframe posee por fila el dato de la frecuencia para cada clase. \n",
        "word_classes_df.fillna(0,inplace=True)\n",
        "\n",
        "#Obtenemos el nuevo dataframe\n",
        "featured_test=pd.concat([test,word_classes_df],axis=1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw7FpzjwQT7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featured_test.to_csv('featured_test.csv', sep=',')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CrG8ac5SSmA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}